{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from collections import deque \n",
    "from itertools import product\n",
    "from dqn import DQN\n",
    "from memory import Memory\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self,priority_queue,tdm_cycle):\n",
    "        self.model = DQN(priority_queue,tdm_cycle)\n",
    "        self.state_size = priority_queue\n",
    "        self.action_size = 2**tdm_cycle\n",
    "        self.epsilon = 0.999\n",
    "        self.epsilon_min = 0.01\n",
    "        self.step = 1\n",
    "        self.test = False\n",
    "        self.step_decrease = 5000 #step_decrease마다 epsilon 감소, 전체 episode를 약 10번으로 나눠서 epsilon을 감소\n",
    "        self.discount_factor = 0.8 #할인율. 1에 가까울 수록 미래에 받는 보상도 중요, 0에 가까울수록 즉각적인 보상이 중요\n",
    "        self.memory = Memory\n",
    "        self.mse_loss = []\n",
    "        \n",
    "    def choose_action(self,state):\n",
    "        if np.random.random() < self.epsilon: \n",
    "            return format(random.randrange(self.action_size),'0'+str(tdm_cycle)+'b')\n",
    "        else:\n",
    "            return format(np.argmax(self.model.predict_one(state)),'0'+str(tdm_cycle)+'b') #tdm cycle자리수의 이진수 변환 '111111'\n",
    "        \n",
    "    def epsilon_decay(self):\n",
    "        \n",
    "        if self.test:\n",
    "            self.epsilon = self.epsilon_min\n",
    "        \n",
    "        else:\n",
    "            if (self.step%self.step_decrease ==0):\n",
    "                self.epsilon = max(self.epsilon_min,pow(self.epsilon,int(self.step/self.step_decrease+1)))\n",
    "        \n",
    "        print (\"action 선택\", self.epsilon)\n",
    "       \n",
    "        self.step +=1 \n",
    "        \n",
    "        print (\"action 선택 횟수(step)\" , self.step , self.epsilon)\n",
    "        \n",
    "        return self.step,self.epsilon\n",
    "        \n",
    "    #agent memory\n",
    "    \n",
    "    def observation(self, sample): #sample = [state,action,reward,next_state,done]\n",
    "        self.memory.remember(sample)\n",
    "\n",
    "    def state_target(self, batch): #sample을 받아서 dqn의 input(state)과 target(predicted q-value)로 데이터셋을 나눠주는작업\n",
    "        batch_len = len(batch)\n",
    "\n",
    "        states = np.array([o[0] for o in batch])\n",
    "        states_ = np.array([o[3] for o in batch]) #next state\n",
    "\n",
    "        p = self.model.predict(states) #model predict with state\n",
    "        pTarget_ = self.model.predict(states_, target=True)#target_model predict with next state\n",
    "\n",
    "        x = np.zeros((batch_len, self.state_size)) \n",
    "        y = np.zeros((batch_len, self.action_size))\n",
    "        errors = np.zeros(batch_len)\n",
    "\n",
    "        for i in range(batch_len):\n",
    "            o = batch[i]\n",
    "            s = o[0]\n",
    "            a = int(o[1],2) #binary action을 action id로 matching\n",
    "            r = o[2]\n",
    "            s_ = o[3]\n",
    "            done = o[4]\n",
    "\n",
    "            t = p[i]\n",
    "            old_value = t[a]\n",
    "            if done:\n",
    "                t[a] = r\n",
    "            else:\n",
    "                t[a] = r + self.discount_factor * np.amax(pTarget_[i])\n",
    "\n",
    "            x[i] = s\n",
    "            y[i] = t\n",
    "\n",
    "        return [x, y]\n",
    "\n",
    "    # build the replay buffer \n",
    "    def replay(self,allowed_actions):\n",
    "        \n",
    "        batch_size = 32\n",
    "        if len(self.memory) < batch_size: #buffer에 저장된 memory가 buffer의 총 batch_size보다 작다면 return\n",
    "            return \n",
    "        \n",
    "        batch = self.memory.sample(self.batch_size)\n",
    "        x, y = self.state_target(batch)\n",
    "        self.model.train(x, y)\n",
    "        \n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.model.update_target_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jhtf-gpu",
   "language": "python",
   "name": ".env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
